# ====================================================
# @Time    : 2/25/19 9:11 AM
# @Author  : Xiao Junbin
# @Email   : junbin@comp.nus.edu.sg
# @File    : video_object_detection.py
# ====================================================
import numpy as np

from common import voc_ap, iou



def trajectory_overlap(gt_trajs, pred_traj):
    """
    Calculate overlap among trajectories
    """
    max_overlap = 0
    max_index = -1
    thresh = 0.5
    for t, gt_traj in enumerate(gt_trajs):
        top1 = 0
        total = len(set(gt_traj.keys()) | set(pred_traj.keys()))
        for i, fid in enumerate(gt_traj):
            if fid not in pred_traj:
                continue
            sIoU = iou(gt_traj[fid], pred_traj[fid])
            if sIoU >= thresh:
                top1 += 1

        # tIoU = (top1 + top2 + top3) * 1.0 / (3 * total)
        tIoU = (top1) * 1.0 / (total)

        if tIoU > max_overlap:
            max_index = t
            max_overlap = tIoU

    return max_overlap, max_index


def evaluate(gt, pred, use_07_metric=True, thresh_t=0.5):
    """
    Evaluate the predictions
    """
    gt_classes = set()
    for vid, tracks in gt.items():
        for traj in tracks:
            gt_classes.add(traj['category'])
    gt_class_num = len(gt_classes)

    result_class = dict()
    for vid, tracks in pred.items():
        for traj in tracks:
            traj['vid'] = vid
            traj['viou'] = 0
            traj['hit_tid'] = -1
            if traj['category'] not in result_class:
                result_class[traj['category']] = [traj]
            else:
                result_class[traj['category']].append(traj)

    ap_class = dict()
    recall_class = dict()
    print('Computing average precision AP over {} classes...'.format(gt_class_num))
    for c in gt_classes:
        if c not in result_class: 
            ap_class[c] = 0.
            recall_class[c] = 0
            continue
        npos = 0
        class_recs = {}

        # collect gt
        for vid in gt:
            gt_trajs = [trk['trajectory'] for trk in gt[vid] if trk['category'] == c]
            gt_tids = [trk['tid'] for trk in gt[vid] if trk['category'] == c]
            det = [False] * len(gt_trajs)
            npos += len(gt_trajs)
            class_recs[vid] = {'trajectories': gt_trajs, 'det': det, 'tids': gt_tids}

        # collect pred
        trajs = result_class[c]
        nd = len(trajs)
        fp = np.zeros(nd)
        tp = np.zeros(nd)

        scores = np.array([trj['score'] for trj in trajs])
        vids = [trj['vid'] for trj in trajs]
        sorted_inds = np.argsort(-scores)
        sorted_vids = [vids[id] for id in sorted_inds]
        sorted_traj = [trajs[id] for id in sorted_inds]

        for d in range(nd):
            R = class_recs[sorted_vids[d]]
            gt_trajs = R['trajectories']
            gt_tids = R['tids']
            pred_traj = sorted_traj[d]
            max_overlap, max_index = trajectory_overlap(gt_trajs, pred_traj['trajectory'])

            if max_overlap > 0:
                pred_traj['viou'] = max_overlap
                pred_traj['hit_tid'] = gt_tids[max_index]

            if max_overlap >= thresh_t:
                if not R['det'][max_index]:
                    tp[d] = 1.
                    R['det'][max_index] = True
                else:
                    fp[d] = 1.
            else:
                fp[d] = 1.

        # compute precision recall
        fp = np.cumsum(fp)
        tp = np.cumsum(tp)

        rec = tp / float(npos)
        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
        ap = voc_ap(rec, prec, use_07_metric)

        ap_class[c] = ap

        gt_sum = 0
        gt_hit = 0
        for vid in class_recs:
            gt_sum += len(class_recs[vid]['det'])
            for hit in class_recs[vid]['det']:
                if hit:
                    gt_hit += 1
        recall_class[c] = gt_hit * 1.0 / gt_sum

    # compute mean ap and print
    print('=' * 30)
    ap_class = sorted(ap_class.items(), key=lambda ap_class: ap_class[0])
    total_ap = 0.
    for i, (category, ap) in enumerate(ap_class):
        rec = recall_class[category]
        print('{:>2}{:>20}\t{:.4f}\t{:.4f}'.format(i+1, category, ap, rec))
        total_ap += ap
    mean_ap = total_ap / gt_class_num 
    print('=' * 30)
    print('{:>22}\t{:.4f}'.format('mean AP', mean_ap))

    return mean_ap, ap_class


if __name__ == "__main__":
    """
    You can directly run this script from the parent directory, e.g.,
    python -m evaluation.video_object_detection val_object_groundtruth.json val_object_prediction.json
    """
    import json
    from argparse import ArgumentParser

    # parser = ArgumentParser(description='Video object detection evaluation.')
    # parser.add_argument('groundtruth', type=str, help='A ground truth JSON file generated by yourself')
    # parser.add_argument('prediction', type=str, help='A prediction file')
    # args = parser.parse_args()
    # groundtruth_path = args.groundtruth
    # prediction_path = arg.prediction

    method = 'vot'
    groundtruth_path = '../data/vidor_hoid_mini/vidor_hoid_mini_val_object_gt.json'
    prediction_path = '../output/vidor_hoid_mini/fgfa_det_%s.json' % method

    print('Loading ground truth from {}'.format(groundtruth_path))
    with open(groundtruth_path, 'r') as fp:
        gt = json.load(fp)
    print('Number of videos in ground truth: {}'.format(len(gt)))

    print('Loading prediction from {}'.format(prediction_path))
    with open(prediction_path, 'r') as fp:
        pred = json.load(fp)
    print('Number of videos in prediction: {}'.format(len(pred['results'])))

    mean_ap, ap_class = evaluate(gt, pred['results'])

    with open(prediction_path, 'w') as fp:
        print('updating predictions...')
        json.dump(pred, fp)
